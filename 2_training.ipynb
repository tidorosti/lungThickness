{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the U-Net\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@authors M. Schultheiss, T. Dorosti\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from scipy import ndimage\n",
    "from skimage.transform import rescale, resize\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.layers import Conv2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras.initializers\n",
    "from keras.models import load_model\n",
    "\n",
    "from UNet import UNet\n",
    "from pyDeleClasses import Slice2D, Slice2DSet\n",
    "from configuration_paper import cparam, CURRENT_CONFIG, PATH_SERVER_CACHE, POSITION\n",
    "from functions import getSyntheticData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup GPU\n",
    "CURRENT_GPU = '?'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(CURRENT_GPU)\n",
    "\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())\n",
    "print(tf.test.is_gpu_available('GPU:{}'.format(CURRENT_GPU)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get Input Synthetic Radiographs Data and Lung thickness Ground Truth Lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.A) Luna16 Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataPath = '???' # path to metadata .csv file\n",
    "dataset_name = 'luna16'\n",
    "radiograph_range = range(0,10) # get all 10 projections for luna\n",
    "slices_luna = getSyntheticData(metadataPath, PATH_SERVER_CACHE, POSITION, dataset_name, CURRENT_CONFIG, radiograph_range)\n",
    "tx_luna, ty_luna, _, _, valx_luna, valy_luna = slices_luna.split_data_by_csv(\"splits/lungvolume\") # dont save the test data in the training script\n",
    "\n",
    "print(\"Train Len\", len(tx_luna))\n",
    "print(\"Val Len\", len(valx_luna))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.B) PE Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadataPath = '???' # path to metadata .csv file\n",
    "dataset_name = 'PE'\n",
    "radiograph_range = range(4,5) # get the central projections for PE\n",
    "slices_PE = getSyntheticData(metadataPath, PATH_SERVER_CACHE, POSITION, dataset_name, CURRENT_CONFIG, radiograph_range)\n",
    "tx_PE, ty_PE, _, _, valx_PE, valy_PE = slices_PE.split_data_by_csv(\"splits/lungvolumePE\") # dont save the test data in the training script\n",
    "\n",
    "print(\"Train Len\", len(tx_PE))\n",
    "print(\"Val Len\", len(valx_PE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# combine the datasets for the final train and validation data:      \n",
    "tx, ty, valx, valy= [],[],[],[]\n",
    "tx = tx_luna + tx_PE\n",
    "ty = ty_luna + ty_PE\n",
    "valx = valx_luna + valx_PE \n",
    "valy = valy_luna + valy_PE\n",
    "\n",
    "print(\"Train Len\", len(tx))\n",
    "print(\"Val Len\", len(valx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set up model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data generator\n",
    "def generator(x_train, y_train, batch_size, data_gen_args, start_seed):\n",
    "    datagen = ImageDataGenerator(\n",
    "        **data_gen_args).flow(x_train, x_train, batch_size, seed=start_seed)\n",
    "    maskgen = ImageDataGenerator(\n",
    "        **data_gen_args).flow(y_train, y_train, batch_size, seed=start_seed)\n",
    "    while True:\n",
    "        batchx, _ = datagen.next()\n",
    "        batchy, _ = maskgen.next()\n",
    "        yield batchx, batchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at some augmentations:\n",
    "data_gen_args = dict(featurewise_center=False,\n",
    "                     featurewise_std_normalization=False,\n",
    "                     rotation_range=10.0,\n",
    "                     height_shift_range=0.20,\n",
    "                     width_shift_range=0.20,\n",
    "\n",
    "                     fill_mode='constant',\n",
    "                     zoom_range=0,\n",
    "                     cval=0.,\n",
    "                     horizontal_flip=False)\n",
    "\n",
    "batch_size = 8\n",
    "train_gen = generator(np.array(tx)[:,:,:,np.newaxis], np.array(ty)[:,:,:,np.newaxis], batch_size, data_gen_args, 42)\n",
    "for i in range(0,3):\n",
    "    a, b = next(train_gen)\n",
    "    #print(a.shape, b.shape)\n",
    "i=3\n",
    "plt.subplot(121)\n",
    "plt.imshow(a[i], cmap='gray')\n",
    "plt.colorbar()\n",
    "plt.subplot(122)\n",
    "plt.imshow(b[i], cmap='gray')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = UNet(slice_shape=[cparam(\"IMAGEWIDTH\"), cparam(\"IMAGEWIDTH\"), 1],  layer_depth=6, filter_count=32, kernel_size_down=(3,3), kernel_size_pool=(2, 2), dilation_rates=[1,2,1,2,1,1,], dropout=False,activation=\"relu\").get_keras_model() \n",
    "model_name = '???'\n",
    "checkpoint_dir='???'\n",
    "loss_function = \"mean_squared_error\" \n",
    "monitor =\"mean_squared_error\"\n",
    "opt = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "epochs = 120 \n",
    "logdir = os.path.join(\"logs_unet\", model_name+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "model.compile(loss=loss_function,\n",
    "              optimizer=opt,\n",
    "              metrics=[monitor])\n",
    "            \n",
    "checkpoint = ModelCheckpoint(checkpoint_dir+'/{}_model.h5'.format(model_name), monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n",
    "data_gen_args = dict(featurewise_center=False,\n",
    "                     featurewise_std_normalization=False,\n",
    "                     rotation_range=10.0,\n",
    "                     height_shift_range=0.20,\n",
    "                     width_shift_range=0.20,\n",
    "                     fill_mode='constant',\n",
    "                     zoom_range=0,\n",
    "                     cval=0.,\n",
    "                     horizontal_flip=False)\n",
    "    \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(generator(np.array(tx)[:,:,:,np.newaxis], np.array(ty)[:,:,:,np.newaxis], batch_size, data_gen_args, 42),\n",
    "                              validation_data=generator(np.array(valx)[:,:,:,np.newaxis], np.array(valy)[:,:,:,np.newaxis], batch_size, data_gen_args, 42),\n",
    "                              steps_per_epoch=len(tx)//batch_size, epochs=epochs,\n",
    "                              validation_steps=len(valx)//batch_size, callbacks=[checkpoint, tensorboard_callback]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the history to plot loss later\n",
    "# convert the history.history dict to a pandas DataFrame:     \n",
    "hist_df = pd.DataFrame(history.history) \n",
    "# save to csv: \n",
    "run='?'\n",
    "hist_csv_file = os.path.join(PATH_SERVER_CACHE,CURRENT_CONFIG)+'{}_modelHistory_{}.csv'.format(model_name, run)\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    hist_df.to_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the loss\n",
    "loss = 'val_loss'\n",
    "plt.plot(hist_df[loss], label='val')\n",
    "plt.plot(hist_df['loss'], label='train')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.ylabel(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
